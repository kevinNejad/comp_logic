{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"a member of the human race\"\n",
    "b=\"a complex system\"\n",
    "c=\"a person who shares their knowledge\"\n",
    "d=\"a person who's occupation is teaching\"\n",
    "e=\"a common employee of a university\"\n",
    "\n",
    "document = d\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(sent) for sent in [document]]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag = [nltk.pos_tag(sent) for sent in tokens][0]\n",
    "postag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        {<RB.?>*<VB.?>*<JJ>*<VB.?>+<VB>?} # Verbs and Verb Phrases\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(postag)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label() =='NP'):\n",
    "        yield subtree.leaves()\n",
    "        \n",
    "def get_word_postag(word):\n",
    "    if pos_tag([word])[0][1].startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    if pos_tag([word])[0][1].startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    if pos_tag([word])[0][1].startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    postag = get_word_postag(word)\n",
    "    word = lemmatizer.lemmatize(word,postag)\n",
    "    return word\n",
    "\n",
    "def get_terms(tree):    \n",
    "    for leaf in leaves(tree):\n",
    "        terms = [normalise(w) for w,t in leaf]\n",
    "        yield terms\n",
    "\n",
    "terms = get_terms(tree)\n",
    "\n",
    "features = []\n",
    "for term in terms:\n",
    "    _term = ''\n",
    "    for word in term:\n",
    "        _term += ' ' + word\n",
    "    features.append(_term.strip())\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stanfordcorenlp import StanfordCoreNLP\n",
    "# from nltk.tree import Tree\n",
    "\n",
    "# nlp = StanfordCoreNLP('/path/to/stanford-corenlp-full-2018-10-05')\n",
    "\n",
    "# sentence = 'Who drives a tractor?'\n",
    "\n",
    "\n",
    "# def extract_phrase(tree_str, label):\n",
    "#     phrases = []\n",
    "#     trees = Tree.fromstring(tree_str)\n",
    "#     for tree in trees:\n",
    "#         for subtree in tree.subtrees():\n",
    "#             if subtree.label() == label:\n",
    "#                 t = subtree\n",
    "#                 t = ' '.join(t.leaves())\n",
    "#                 phrases.append(t)\n",
    "\n",
    "#     return phrases\n",
    "\n",
    "\n",
    "# tree_str = nlp.parse(sentence)\n",
    "\n",
    "# print(tree_str)\n",
    "# # u'(ROOT\\n  (SBARQ\\n    (WHNP (WP Who))\\n    (SQ\\n      (VP (VBZ drives)\\n        (NP (DT a) (NN tractor))))\\n    (. ?)))'\n",
    "\n",
    "# nps = extract_phrase(tree_str, 'NP')\n",
    "# print(nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"a member of the human race\"\n",
    "b=\"a complex system\"\n",
    "c=\"a person who shares their knowledge\"\n",
    "d=\"a person who's occupation is teaching\"\n",
    "e=\"a common employee of a university\"\n",
    "f = \"beautiful\"\n",
    "document = f\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_text = (document)\n",
    "introduction_doc = nlp(introduction_text)\n",
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_text = (document)\n",
    "conference_doc = nlp(conference_text)\n",
    "pos = []\n",
    "for token in conference_doc:\n",
    "    print('token: ', token)\n",
    "#     print('tag: ',token.tag_)ps\n",
    "#     print('pos: ', token.pos_)\n",
    "    pos.append(token.pos_)\n",
    "print('POS: ', pos)\n",
    "# Extract Noun Phrases\n",
    "# for chunk in conference_doc.noun_chunks:\n",
    "#     print (chunk)z\n",
    "print('CHUNK: ', [chunk for chunk in conference_doc.noun_chunks])\n",
    "print('PATTERN')\n",
    "\n",
    "about_talk_text = (document)\n",
    "NN_pattern = r'(<DET>?<NOUN>)'\n",
    "CNP_pattern = r'(<DET>?<ADJ>?<NOUN><ADP>?<DET>?<ADJ>?<NOUN>?)'\n",
    "about_talk_doc = textacy.make_spacy_doc(about_talk_text,\n",
    "                                        lang='en_core_web_sm')\n",
    "verb_phrases = textacy.extract.pos_regex_matches(about_talk_doc, CNP_pattern)\n",
    "# Print all Verb Phrase\n",
    "for chunk in verb_phrases:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "about_talk_text = (document)\n",
    "pattern = r'(<VERB>?<ADV>*<VERB>+)'\n",
    "about_talk_doc = textacy.make_spacy_doc(about_talk_text,\n",
    "                                        lang='en_core_web_sm')\n",
    "verb_phrases = textacy.extract.pos_regex_matches(about_talk_doc, pattern)\n",
    "# Print all Verb Phrase\n",
    "for chunk in verb_phrases:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"a member of the human race\"\n",
    "b=\"a complex system\"\n",
    "c=\"a person who shares their knowledge\"\n",
    "d=\"a person who's occupation is teaching\"\n",
    "e=\"a common employee of a university\"\n",
    "f=\"a person\"\n",
    "g=\"person\"\n",
    "\n",
    "text = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_pattern = r'(<DET>?<NOUN>)'\n",
    "VB_pattern = r''\n",
    "\n",
    "CNP_pattern = r'(<DET>?<ADJ>?<NOUN><ADP>?<DET>?<ADJ>?<NOUN>?)'\n",
    "CVP_patttern = r''\n",
    "\n",
    "spacy_doc = textacy.make_spacy_doc((text),\n",
    "                                    lang='en_core_web_sm')\n",
    "\n",
    "noun_reg_check = [c for c in textacy.extract.matches(spacy_doc, NN_pattern)]\n",
    "verb_reg_check = None\n",
    "\n",
    "CNP_reg_check = [c for c in textacy.extract.matches(spacy_doc, CNP_pattern)]\n",
    "# CVP_reg_check = textacy.extract.matches(spacy_doc, CVP_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_reg_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'<VERB>*<ADV>*<VERB>+<PART>*'\n",
    "verb_pattern = [{\"POS\": \"VERB\", \"OP\": \"*\"},{\"POS\": \"ADV\", \"OP\": \"*\"},{\"POS\": \"VERB\", \"OP\": \"+\"},{\"POS\": \"PART\", \"OP\": \"*\"}]\n",
    "\n",
    "d = \"You no longer need to be at the station on sundays.\"\n",
    "nlp_t = textacy.make_spacy_doc((d),\n",
    "                                    lang='en_core_web_sm')\n",
    "t_list_0 = textacy.extract.pos_regex_matches(nlp_t, pattern)\n",
    "t_list_1 = textacy.extract.matches(nlp_t, verb_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/Documents/UofB/comp_log/cw/prolexa/prolexa-prolexa-plus/cp_venv/lib/python3.8/site-packages/textacy/extract.py:334: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  utils.deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[no longer need to]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t_list_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_regex(text_doc ,pattern):\n",
    "    matches_ = list(textacy.extract.matches(nlp_t, verb_pattern))\n",
    "    matches_\n",
    "    seen_se = set()\n",
    "    filtered_matches = []\n",
    "    for match in sorted(matches_, key=len, reverse=True):\n",
    "        s, e = match.start, match.end\n",
    "        if any(s >= ms and e <= me for ms, me in seen_se):\n",
    "            continue\n",
    "        else:\n",
    "            seen_se.add((s, e))\n",
    "            filtered_matches.append(match)\n",
    "    return sorted(filtered_matches, key=lambda m: m.start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[no longer need to]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_pos_regex(nlp_t,verb_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[no longer need to]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_ = list(textacy.extract.matches(nlp_t, verb_pattern))\n",
    "matches_\n",
    "seen_se = set()\n",
    "filtered_matches = []\n",
    "for match in sorted(matches_, key=len, reverse=True):\n",
    "    s, e = match.start, match.end\n",
    "    if any(s >= ms and e <= me for ms, me in seen_se):\n",
    "        continue\n",
    "    else:\n",
    "        seen_se.add((s, e))\n",
    "        filtered_matches.append(match)\n",
    "sorted(filtered_matches, key=lambda m: m.start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complex_tag(label):\n",
    "    # Grammar patterns regex\n",
    "    NN_pattern=[{'POS':\"DET\",\"OP\":\"?\"},{'POS':\"NOUN\",\"OP\":\"!\"}]\n",
    "    #NN_pattern = r'(<DET>?<NOUN>)'\n",
    "    #VB_pattern = r''\n",
    "\n",
    "    CNP_pattern=[\n",
    "            {\"POS\":\"DET\",\"OP\":\"?\"},{\"POS\":\"ADJ\",\"OP\":\"?\"},{\"POS\":\"NOUN\"},\n",
    "            {\"POS\":\"ADP\",\"OP\":\"?\"},{\"POS\":\"DET\",\"OP\":\"?\"},{\"POS\":\"ADJ\",\"OP\":\"?\"},\n",
    "            {\"POS\":\"NOUN\",\"OP\":\"?\"}]\n",
    "    #CNP_pattern = r'(<DET>?<ADJ>?<NOUN><ADP>?<DET>?<ADJ>?<NOUN>?)'\n",
    "    #CVP_patttern = r''\n",
    "\n",
    "    spacy_doc = textacy.make_spacy_doc((label),lang='en_core_web_sm')\n",
    "\n",
    "    noun_reg_check = extract_pos_regex(spacy_doc, NN_pattern)\n",
    "    if len(noun_reg_check) > 0:\n",
    "    \n",
    "        print('noun_reg_check: ', noun_reg_check, noun_reg_check[0].text.strip()==label.strip())\n",
    "    \n",
    "    verb_reg_check = None\n",
    "\n",
    "    CNP_reg_check = extract_pos_regex(spacy_doc, CNP_pattern)\n",
    "    if len(CNP_reg_check) > 0:\n",
    "        print('CNP_reg_check: ', CNP_reg_check, CNP_reg_check[0].text.strip()==label.strip())\n",
    "    \n",
    "    \n",
    "    CVP_reg_check = None\n",
    "\n",
    "\n",
    "def extract_pos_regex(text_doc ,pattern):\n",
    "    matches_ = list(textacy.extract.matches(text_doc, pattern, ))\n",
    "    matches_\n",
    "    seen_se = set()\n",
    "    filtered_matches = []\n",
    "    for match in sorted(matches_, key=len, reverse=True):\n",
    "        s, e = match.start, match.end\n",
    "        if any(s >= ms and e <= me for ms, me in seen_se):\n",
    "            continue\n",
    "        else:\n",
    "            seen_se.add((s, e))\n",
    "            filtered_matches.append(match)\n",
    "    return sorted(filtered_matches, key=lambda m: m.start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_reg_check:  [a common, of, a] False\n",
      "CNP_reg_check:  [a common employee of a university] True\n"
     ]
    }
   ],
   "source": [
    "a=\"a member of the human race\"\n",
    "b=\"a complex system\"\n",
    "c=\"a person who shares their knowledge\"\n",
    "d=\"a person who's occupation is teaching\"\n",
    "e=\"a common employee of a university\"\n",
    "f=\"a person\"\n",
    "g=\"educator\"\n",
    "\n",
    "label = e\n",
    "get_complex_tag(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
